% -*- coding: utf-8 -*-
\documentclass[a4paper,10pt,DIV=16]{scrartcl}  
\usepackage{ucs} 
\usepackage{array}
\usepackage[utf8x]{inputenc}                   
\usepackage[T1]{fontenc}
\usepackage{lmodern}                        
\usepackage[english]{babel} 
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[pdftex]{graphicx}
\usepackage{graphicx,amsfonts,amssymb,amsmath,amsthm}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage[normalem]{ulem}
\hypersetup{colorlinks=true,hypertexnames=false}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage[]{tcolorbox}
\usepackage{paralist}
\usepackage{tikz}
\usepackage{pgfplots}

\subject{Response to the reviewers}
\title{Large-scale feature selection with Gaussian mixture models for the classification of high dimensional remote sensing images}
\date{\today}
\author{Adrien Lagrange \and Mathieu Fauvel \and Manuel Grizonnet}

\newcommand{\rev}[1]{\textcolor{magenta}{#1}}
%%% Definition of the boxes
\newtcolorbox[auto counter,number within=section]{revbox}[1][]{%
colback=red!5!white,colframe=red!50!white,fonttitle=\sffamily\bfseries,
title=Comment~\thetcbcounter: #1}
\newtcolorbox[auto counter,number within=section]{resbox}[1][]{%
colback=blue!5!white,colframe=blue!5!white,title=Response~:,coltitle=black,fonttitle=\sffamily\bfseries}

\begin{document}
\maketitle

We have revised  the paper taking into account the  comments raised by
the reviewers.   We provide an  item-by-item response detailed  in the
following.   We hope  that this  improved  paper is  now suitable  for
publication  in  {\sc  Special Issue on Computational Imaging for Earth Sciences}.   We would  like  to  thank  the
anonymous reviewers for their dedication in reviewing our work and for
their constructive comments.

For clarity,  the modification appear  in \textcolor{magenta}{magenta}
in the revised paper.


\section{Reviewer 1}

\begin{revbox}
In Algorithm 3, a threshold EPS\_FLT on eigenvalues is used to prevent numerical instability. It’s not clear how the authors select the proper value for this threshold. And how does this threshold influence the classification accuracy?

\begin{resbox}
  We acknowledge for the missing information. Numerical instability comes from the computation of the inverse and of the log-determinant of the covariance matrix. When the matrix is badly conditionned, e.g. when it is estimated with less samples than the number of parameters to estimate, some eigenvalues might be negative or null. Therefore the covariance matrix is non-invertible and its determinant is zero. To prevent such situations, the eigenvalues are shrunk.

  In practice, it consists in thresholding eigenvalues to the smallest significant positive value: $\lambda_c = \max\{EPS\_FLT,\lambda_c\}$, where $EPS\_FLT$ is defined by the computer architecture and C++ specifications. This thresholding can be seen as a ridge regularization with a very small value.

  In the context of feature selection, it is not expected to increase the classification accuracy. It rather ensures the computation to be tractable.

  The paper has been changed to make the sentence more clear: \emph{To
    prevent  numerical  instability,   \rev{non  strictly  positive
      eigenvalues are thresholded to a fixed value EPS\_FLT}.}
  \end{resbox}

\end{revbox}

\begin{revbox}
  The authors show the experimental results on two different types of hyper-dimensional images, which is very good. But I have some doubts on the construction of the second high dimensional cube where 191 features are constructed based on the low dimensional RGBIR images. Are these features simply used for the purpose of increasing the data dimensionality? Or are there benefits in terms of classification accuracy?
  \begin{resbox}
    The extraction of spatial features from very high spatial resolution images is a standard practice in remote sensing. There are a vast literature, let us cite for instance the \emph{morphological profile} and its extension, all the possible spectral features such as \emph{NDVI} and recently all the convolutional filters. There is in general a significant increase of the classification accuracy.

    However, the reviewer is right, there is also an increase of the data dimensionality which can result in many statistical issues. Usually, the choice of features is performed by experts. In this work, to illustrate the potential of the feature selection, a large number of features is extracted and the optimal selection is done by the algorithm, as in ref [26], thus alleviate the \emph{curse of dimensionality}.

    To make it clear, the following part of the paper has been modified:

    \emph{\rev{Conventionally,  the following  features are  extracted using  the
      RGBIR  images in  order to  increase the  classification accuracy,
      similarly to [26]:}\\
    $\vdots$\\    
    The normalized DSM and the raw  RGBIR image are added to these 191
    features and  then stacked to create  a new image with  196 bands.
    \rev{The  resulting  data cube  is  therefore  high-dimensional.}}
    
  \end{resbox}
\end{revbox}


\begin{revbox}
  In Fig. 5, the optimal number of variables are selected if the criterion stops to increase significantly. It is unclear to me how to make this selection. For example in Fig. 5, I would pick the optimal number of variables to be 7 instead of 18 which the authors suggest, since after 7 variables I do not observe a significant increase. Maybe plotting the discrete derivative of criteria together with Fig. 5 can help in the explanation.
  \begin{resbox}
    We would like to thank the reviewer for this very important comment. The sentence was indeed not clear.  In order to check if the criterion stops to increase significantly, we compute the discrete derivative between two iterations and normalize it by its maximum value. The number of selected features corresponds to the last iteration before the value drops below $10^{-3}$ for all datasets.

    The derivative for the example presented on Fig. 5 of the paper is presented in Figure~\ref{fig:deriv}.  For this example, 7 features gives a kappa of $0.620$ and 18 features a kappa of $0.657$. This figure was not included in the paper in order to keep the paper not too long. If the reviewer asks, it is of course possible to include it in the paper.

    The paper has been updated as follow:

    \rev{It  is found  by  computing the  discrete  derivative of  the
      criteria  between  two  iterations  and normalizing  it  by  its
      maximum value.   The number of selected  features corresponds to
      the last  iteration before the  value drops below  $10^{-3}$ for
      all datasets}.

  \end{resbox}
\end{revbox}

\begin{figure}[!ht]
  \centering
  \begin{tikzpicture}
    \begin{axis}[ymin=0,ymax=0.25,grid,axis x line=left,axis y line=left,xlabel={\# variables},ylabel={Derivative of\\Cohen's kappa},ylabel style={align=center},yticklabel style={/pgf/number format/fixed,/pgf/number format/precision=3}]
      \addplot[thick,black] table[x expr=\coordindex,y=deriv] {derivate_criterion_1000spl_aisa.txt};
      \draw[thick,red] (axis cs:18,\pgfkeysvalueof{/pgfplots/ymin}) -- (axis cs:18,\pgfkeysvalueof{/pgfplots/ymax});
      \draw[thick,black] (axis cs:24,\pgfkeysvalueof{/pgfplots/ymin}) -- (axis cs:24,\pgfkeysvalueof{/pgfplots/ymax});
    \end{axis};
  \end{tikzpicture}
  \caption{Derivative of criterion (kappa) in function of the number of selected variables for first trial with Aisa dataset with 500 samples by class. Red vertical line is the retained number of variables and black vertical line is the maximum of the criterion.\label{fig:deriv}}
\end{figure}


\begin{revbox}
  Following the question 3, why choosing the optimal number of variables based on the increase of criterion instead of the highest value?
  \begin{resbox}
    We also considered this possibility and it is a choice available in the code of the OTB module we provide. However, selecting the maximum value of the criteria without considering the slope of the curve might be hazardous. At some point, adding more feature does not improve significantly the discrimination of the classes, until a too large number of variables is reached and statistical issues arise. It is therefore safer to stop when the increase of criteria stops to be significant.
\end{resbox}
\end{revbox}

% \begin{figure}[!t]
%   \centering
%   \begin{tikzpicture}
%     \begin{axis}[ymin=0.2,ymax=0.9,grid,axis x line=left,axis y line=left,xlabel={\# variables},ylabel={Cohen's kappa}]
%       \addplot[thick,black] table[x=nb,y=crit] {criterion_1000spl.txt};
%     \end{axis};
%   \end{tikzpicture}
%   \caption{Criterion evolution (kappa) in function of the number of selected variables for first trial with Potsdam dataset with 1000 samples by class.\label{fig:crit-evol}}
% \end{figure}

\begin{revbox}
  In the experiment with Aisa dataset, although several numbers of training samples are tested, the number of training samples seems carefully selected by the authors to show the cases where the GMM classifiers perform better. I guess when a larger number of samples are trained, the random forest will have a higher classification rate. And when a smaller number of samples are used, the KNN will have a shorter training time. The GMM only wins in the classification time regardless of the sample number. It’ll be good to show these cases.
  \begin{resbox}
    The reviewer is right regarding the number of training samples. GMM (not restricted to the feature selection case) does not benefit as much as other machine learning algorithms of the increase of the number of training samples. For the feature selection case, it is discussed in ref [27] for instance, with KNN, RF and SVM. Please note it is also discussed in the paper for the Potsdam dataset.
    
    We tried with 50 samples by class and it leads to the same behavior than with 250 samples (see Table~\ref{tab:aisa-otbsimu}). We therefore choose to not include this additional results in the final version of the paper.

    For larger training set,  it is not possible to test with the chosen validation method for the AISA dataset. We use an equal number of samples in each class and 50\% of the samples are used for test. Thus, as the less represented class contains 2107 samples, we are limited to 1053 samples by class with this dataset. Nevertheless, the second dataset illustrates the case and we also add the following comment in the paper to make it clear that RF could perform better with more samples:
    \emph{Random Forest has similar performance in term of classification rate with 1000 samples \rev{ and one could expect to get a better classification rate with RF if more samples were available}.}
  \end{resbox}
\end{revbox}

\begin{table}[!t]
    \centering
    \caption{Average classification accuracy 20 trials (standard deviation in parenthesis).\label{tab:aisa-otbsimu}}
    \begin{tabular}{lcccc}\toprule
         & \multicolumn{4}{c}{\bfseries Cohen's kappa} \\ \cmidrule{2-5}
        \# samples by class & 50 & 250 & 500 & 1000 \\ \midrule

        GMM SFS kappa &  0.612 (0.028) & {\bfseries 0.678 (0.029)} & {\bfseries 0.687 (0.029)} & {\bfseries 0.699 (0.028)} \\
        GMM SFS JM &     0.622 (0.026) & {\bfseries 0.685 (0.030)} & {\bfseries 0.689 (0.030)} & {\bfseries 0.701 (0.029)} \\
        GMM SFFS JM &    0.622 (0.026) & {\bfseries 0.685 (0.030)} & {\bfseries 0.689 (0.030)} & {\bfseries 0.701 (0.029)} \\
        GMM ridge &      0.597 (0.035) & 0.611 (0.040) & 0.620 (0.036) & 0.642 (0.034) \\
        KNN &            0.481 (0.054) & 0.551 (0.035) & 0.563 (0.033) & 0.574 (0.030) \\
        Random Forest &  0.555 (0.031) & 0.645 (0.026) & 0.673 (0.023) & {\bfseries 0.693 (0.023)} \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{revbox}
  Correct some typos, for example:
  \begin{enumerate}
    \item In Proposition 2, in the sentence after the covariance matrix equation, I guess it should be ``u\_c'' instead of ``v\_c''.
    \item Grammar mistakes such as ``a buildings''.
  \end{enumerate}
  \begin{resbox}
    The changes have been done. Thank you for the careful reading.
    
  \end{resbox}
\end{revbox}

\section{Reviewer 2}
\begin{revbox}
  In section III-A, 5 criterion functions are proposed. With 2 feature selection methods (SFS and SFFS), there should be 10 possible configurations. However, in the experimental parts (page 14, line 21-24), results of only three configurations are shown. The authors said “other configurations have been investigated and performs either equally or lower in terms of classification accuracy.” I would suggest that the authors either show the results of more configurations or not introducing those unrelated criterion functions.
  \begin{resbox}
    We agree with the reviewer comment. In order to keep the paper short, we provide the additional results as a supplementary material. The reviewer can find the results in Table~\ref{tab:aisa-otbsimu-all} for AISA dataset and Table~\ref{tab:potsdam-otbsimu-all} for Potsdam.

    The paper has been modified:

    \emph{For the sake of clarity, only the three aforementioned configurations are discussed here \rev{and the results for all other configurations is available in the supplementary material}.}
  \end{resbox}
\end{revbox}

    \begin{table}[!t]
        \centering
        \caption{Average classification accuracy 20 trials with all configurations of criterion and method (standard deviation in parenthesis).\label{tab:aisa-otbsimu-all}}
        \begin{tabular}{lccc}\toprule
             & \multicolumn{3}{c}{\bfseries Cohen's kappa} \\ \cmidrule{2-4}
            \# samples by class & 250 & 500 & 1000 \\ \midrule

            GMM SFS JM &      0.685 (0.030) & 0.689 (0.030) & 0.693 (0.032) \\
            GMM SFS divKL &   0.680 (0.027) & 0.699 (0.028) & 0.708 (0.031) \\
            GMM SFS kappa &   0.682 (0.025) & 0.698 (0.027) & 0.710 (0.029) \\
            GMM SFS OA &      0.682 (0.025) & 0.698 (0.027) & 0.710 (0.028) \\
            GMM SFS F1mean &  0.682 (0.028) & 0.699 (0.027) & 0.710 (0.028) \\
            GMM SFFS JM &     0.685 (0.030) & 0.689 (0.030) & 0.693 (0.032) \\
            GMM SFFS divKL &  0.547 (0.158) & 0.615 (0.113) & 0.636 (0.120) \\
            GMM SFFS kappa &  0.519 (0.126) & 0.537 (0.102) & 0.545 (0.141) \\
            GMM SFFS OA &     0.531 (0.126) & 0.537 (0.093) & 0.560 (0.127) \\
            GMM SFFS F1mean & 0.447 (0.158) & 0.579 (0.097) & 0.529 (0.129) \\
            \bottomrule
        \end{tabular}
    \end{table}

    \begin{table}[!t]
        \centering
        \caption{Average classification accuracy for 1,000 samples by class averaged over 5 trials  with all configurations of criterion and method (standard deviation in parenthesis).\label{tab:potsdam-otbsimu-all}}
        \begin{tabular}{lccc}\toprule
            & {\bfseries 5\_11 (train)} & {\bfseries 5\_12 (test)} & {\bfseries 3\_10 (test)} \\ \cmidrule{2-4}
            GMM SFS JM &      &&\\
            GMM SFS divKL &   &&\\
            GMM SFS kappa &   &&\\
            GMM SFS OA &      &&\\
            GMM SFS F1mean &  &&\\
            GMM SFFS JM &     &&\\
            GMM SFFS divKL &  &&\\
            GMM SFFS kappa &  &&\\
            GMM SFFS OA &     &&\\
            GMM SFFS F1mean & &&\\
            \bottomrule
        \end{tabular}
    \end{table}

\begin{revbox}
  Only Cohen’s Kappa is utilized to analyze the classification accuracy. The experimental results would be more persuasive if other criterion functions are utilized as well. I am interested to see how different methods’ performance varies under different criterion functions.
  \begin{resbox}
    The reviewer is right by suggesting to look at different criteria. Again, in order to keep the length of the paper reasonable, only the Kappa coefficient is reported in the paper. This choice has been motivated by previous publications that recommend this criterion, see for instance ref [34] page 105.

    The other criterion are now provided in the supplementary material, and are given here Table~\ref{tab:aisa-otbsimu-othereval} and  Table~\ref{tab:potsdam-otbsimu-othereval}.

    The paper has been modified:

    \emph{The    classification   rate    is    presented   using    Cohen's kappa \rev{but scores computed with overall accuracy and mean of f1-score are available in supplementary material}.}
  \end{resbox}
\end{revbox}

    \begin{table}[!t]
        \centering
        \caption{Average classification accuracy 20 trials evaluated with overall accuracy and mean of F1-score (standard deviation in parenthesis).\label{tab:aisa-otbsimu-othereval}}
        \begin{tabular}{lcccccc}\toprule
             & \multicolumn{3}{c}{\bfseries Overall Accuracy} & \multicolumn{3}{c}{\bfseries F1-mean} \\ \cmidrule{2-7}
            \# samples by class & 250 & 500 & 1000  & 250 & 500 & 1000 \\ \midrule

            GMM SFS JM &      0.735 (0.027) & 0.738 (0.027) & 0.742 (0.029) & 0.591 (0.018) & 0.596 (0.018) & 0.600 (0.020) \\
            GMM SFS divKL &   0.730 (0.024) & 0.747 (0.026) & 0.754 (0.029) & 0.587 (0.017) & 0.607 (0.017) & 0.616 (0.019) \\
            GMM SFS kappa &   0.733 (0.023) & 0.746 (0.025) & 0.756 (0.027) & 0.591 (0.014) & 0.607 (0.015) & 0.620 (0.018) \\
            GMM SFS OA &      0.732 (0.023) & 0.746 (0.025) & 0.756 (0.026) & 0.591 (0.014) & 0.607 (0.015) & 0.619 (0.018) \\
            GMM SFS F1mean &  0.732 (0.026) & 0.747 (0.025) & 0.757 (0.026) & 0.590 (0.018) & 0.609 (0.016) & 0.620 (0.018) \\
            GMM SFFS JM &     0.735 (0.027) & 0.738 (0.027) & 0.742 (0.029) & 0.591 (0.018) & 0.596 (0.018) & 0.600 (0.020) \\
            GMM SFFS divKL &  0.611 (0.149) & 0.672 (0.105) & 0.690 (0.110) & 0.441 (0.152) & 0.515 (0.114) & 0.541 (0.116) \\
            GMM SFFS kappa &  0.583 (0.119) & 0.599 (0.099) & 0.605 (0.141) & 0.427 (0.117) & 0.439 (0.085) & 0.442 (0.119) \\
            GMM SFFS OA &     0.596 (0.115) & 0.600 (0.090) & 0.620 (0.127) & 0.437 (0.124) & 0.436 (0.079) & 0.455 (0.109) \\
            GMM SFFS F1mean & 0.515 (0.154) & 0.640 (0.093) & 0.590 (0.127) & 0.352 (0.128) & 0.476 (0.085) & 0.431 (0.114) \\
            GMM ridge &       0.665 (0.038) & 0.672 (0.034) & 0.692 (0.033) & 0.580 (0.017) & 0.593 (0.016) & 0.609 (0.018) \\
            KNN &             0.611 (0.033) & 0.622 (0.030) & 0.632 (0.027) & 0.485 (0.019) & 0.502 (0.018) & 0.515 (0.019) \\
            Random Forest &   0.699 (0.023) & 0.723 (0.020) & 0.741 (0.021) & 0.554 (0.018) & 0.578 (0.017) & 0.598 (0.019) \\
            \bottomrule
        \end{tabular}
    \end{table}

    \begin{table}[!t]
        \centering
        \caption{Average classification accuracy for 1,000 samples by class averaged over 5 trials  evaluated with overall accuracy and mean of F1-score (standard deviation in parenthesis).\label{tab:potsdam-otbsimu-othereval}}
        \begin{tabular}{lcccccc}\toprule
             & \multicolumn{2}{c}{\bfseries 5\_11 (train)} & \multicolumn{2}{c}{\bfseries 5\_12 (test)} & \multicolumn{2}{c}{\bfseries 3\_10 (test)} \\ \cmidrule{2-7}
             & {\bfseries OA} & {\bfseries F1-mean} & {\bfseries OA} & {\bfseries F1-mean} & {\bfseries OA} & {\bfseries F1-mean} \\ \cmidrule{2-7}

            GMM SFS JM &      &&&&&\\
            GMM SFS divKL &   &&&&&\\
            GMM SFS kappa &   &&&&&\\
            GMM SFS OA &      &&&&&\\
            GMM SFS F1mean &  &&&&&\\
            GMM SFFS JM &     &&&&&\\
            GMM SFFS divKL &  &&&&&\\
            GMM SFFS kappa &  &&&&&\\
            GMM SFFS OA &     &&&&&\\
            GMM SFFS F1mean & &&&&&\\
            \bottomrule
        \end{tabular}
    \end{table}


\begin{revbox}
  According to Table IV in page 16. GMM SFS JM outperforms GMM SFS kappa a little bit under the kappa criterion. Since GMM SFS kappa updates the features according to the kappa criterion while GMM SFS JM updates the features according to the JM criterion, shouldn’t GMM SFS kappa outperforms GMM SFS JM under the kappa criterion? Or is there some explanation to it?
  \begin{resbox}
    We would like to thank the reviewer for this important comment. First, please note that the score difference between GMM SFS JM and GMM SFS kappa in Table IV is not statistically significant according to the Wilcoxon rank-sum test. Second, the Kappa is estimated by cross-validation during the feature selection, which is a  biased estimators and therefore, it is possible to observe significant differences between the estimated kappa and the true Kappa (usually, a lower value), see ref [36] Chapter 7.

    Our guess is that if the Gaussian hypothesis is a good approximation for the used data, the JM metric performs well  with  performances similar to the ones obtained with the Kappa. It is the case for the AISA data set, while for the Potsdam data, the Kappa coefficient outperformed the JM metric. Our recommendation would be to start with JM since it is fast, and then try Kappa if necessary.
  \end{resbox}
\end{revbox}

\begin{revbox}
  In Table VI and Table VII, the \# of selected features are different for different proposed methods. And the classification performance drops when the \# of selected features decreases. According to Algorithm 2 and Algorithm 3, the \# of selected features should be equal to maxVarNb. So why the \# of selected features are different for different proposed methods? The comparison is not fair to me if the \# of selected features are different. The authors should also show the \# of selected features in Table IV or Table V as well.
  \begin{resbox}
    We apologize  for the misunderstanding,  this part was  unclear in
    the primary  version of  the paper. The  \emph{maxVarNb} parameter
    corresponds  to  the number  of  bands  searched by  the  iterative
    algorithm. To select  the optimal number of  selected features, the
    criterion  evolution is  analyzed, as  explain in  the end  of the
    Section VI.A. The optimal number  of retained features is found by
    computing  the discrete  derivative  of the  criterion between  two
    iterations and normalizing it by  its maximum value. The number of
    selected  features corresponds  to the  last iteration  before the
    value drops below $10^{-3}$ for  all datasets. See response to the
    Reviewer 1 (1.3 and 1.4).

    Therefore, in practice, the optimal number can be different from one method to another.  We believe that it is fair to use a different number of variables for each method because it also reflects the ability of a criterion to identify relevant features.

    The Table V has been modified to include the \# of selected features.
  \end{resbox}
\end{revbox}

\begin{revbox}
  A reference or an explanation should be made in Page 1, line 36 regarding the curse of dimensionality
  \begin{resbox}
    We added the following reference: \emph{D. Donoho, ``High-dimensional data analysis: the curses and blessing of dimensionality,'' in AMS Mathematical challenges of the 21st century, 2000}.
  \end{resbox}
\end{revbox}

\section{Reviewer 3}


\begin{revbox}
  Although the method seems promising, there are some significant issues the authors should address to. The key novelty and contribution of this technique is unfortunately unclear. The authors dedicate a lot of space in explaining already known mathematical formulations of the Gaussian Mixture Models and the feature selection techniques, in comparison with their proposed scheme.
  \begin{resbox}
    We totally agree that some mathematical formulations are already known. Nevertheless, we think that these formulations in this particular context has not been exploited so far. The results of our experiment shows that the proposed algorithm has the advantage to be very efficient in term of processing time with a fairly reasonable cost in term of classification accuracy. It has to be noticed that compared to similar methods using Gaussian Mixture model, no additional hypothesis are made on the covariance matrix which is tractable only because of the efficient implementation.
  \end{resbox}
\end{revbox}

\begin{revbox}
  Additionally, critical references are missing, such as:
  \begin{enumerate}
  \item K. Karalas, G. Tsagkatakis, M. Zervakis, and P. Tsakalides, "Land Classification Using Remotely Sensed Data: Going Multi-Label," IEEE Transactions on Geoscience and Remote Sensing, 2016, doi: 10.1109/TGRS.2016.2520203
  \item K. Karalas, G. Tsagkatakis, M. Zervakis, and P. Tsakalides. "Deep learning for multi-label land cover classification", in SPIE Remote Sensing, Toulouse
  \end{enumerate}

  \begin{resbox}
    After reading with interest this two references, we fail to see the link between these publications and our submission except for the type of data used to test algorithms.

    First reference reviews ways to assign several labels to one sample and so performs spectral unmixing. Second reference focus on the same topic but develop a deep learning approach. Although very interesting, we do not address the subject of multi-labeling in our work.

    According to these observations, we choose to respectfully disagree with the reviewer and take the decision not to add a digression to cite these references.
  \end{resbox}
\end{revbox}

\begin{revbox}
  Another drawback is that the English language is poorly written, since it contains several critical syntax and typographical errors. As a result, some parts of the text are unclarified and misunderstood.
  \begin{resbox}

    We proofread the article to correct as many errors as possible. Thank you for the careful reading.

  \end{resbox}
\end{revbox}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
